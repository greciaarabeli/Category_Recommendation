{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import random\n",
    "import methods\n",
    "import TimeSeries_Clustering\n",
    "from tqdm import tqdm \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tslearn.clustering import KShape\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from tslearn.metrics import sigma_gak, cdist_gak\n",
    "from sklearn.metrics.cluster import adjusted_rand_score\n",
    "import random\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "import xgboost\n",
    "import gc\n",
    "import Evrecsys\n",
    "import sys\n",
    "from itertools import combinations, groupby\n",
    "from collections import Counter\n",
    "from scipy import sparse\n",
    "from lightfm import LightFM\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import Apriori\n",
    "import lightfm_form\n",
    "from lightfm.evaluation import precision_at_k\n",
    "from lightfm.evaluation import recall_at_k\n",
    "from lightfm import LightFM\n",
    "import TimeSeries_Clustering\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Data\n",
    "merchants = pd.read_csv('merchants.csv')\n",
    "data = pd.read_csv('data_cards_0.csv',parse_dates=['purchase_date'])\n",
    "test = pd.read_csv('test.csv', parse_dates=[\"first_active_month\"])\n",
    "train = pd.read_csv('train.csv', parse_dates=[\"first_active_month\"])\n",
    "card_list=data.card_id.unique()\n",
    "test= test[test.card_id.isin(card_list)]\n",
    "train= train[train.card_id.isin(card_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_merch(train, val,test,data):\n",
    "    train=train.merge(data, on='card_id', how='left')\n",
    "    train_avgtarget = train.groupby([\"merchant_id\"])[\"target\"].aggregate(\"mean\").reset_index()\n",
    "    train=train.merge(train_avgtarget, on='merchant_id', how='left')\n",
    "    val=val.merge(data, on='card_id', how='left')\n",
    "    val=val.merge(train_avgtarget, on='merchant_id', how='left').fillna(train_avgtarget.target.mean())\n",
    "    pred=val.groupby([\"card_id\"])[\"target_y\"].aggregate(\"mean\").reset_index()\n",
    "    pred=pred.merge(val[['card_id','target_x']], on='card_id',how='left').drop_duplicates()\n",
    "    return np.sqrt(mean_squared_error(pred.target_y, pred.target_x))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4259014375403916"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_merch(train_df, val_df,test,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Last Merchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def last_merch(train, val,test,data):\n",
    "    train=train.merge(data, on='card_id', how='left')\n",
    "    train_avgtarget = train.groupby([\"merchant_id\"])[\"target\"].aggregate(\"mean\").reset_index()\n",
    "    val=val.merge(data, on='card_id', how='left')\n",
    "    val=val.merge(train_avgtarget, on='merchant_id', how='left').fillna(train_avgtarget.target.mean())\n",
    "    idx_new=val.groupby(['card_id'])['purchase_date'].transform(max) == val['purchase_date']\n",
    "    pred=val[idx_new]\n",
    "    return np.sqrt(mean_squared_error(pred.target_y, pred.target_x))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.507330183890826"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_merch(train_df, val_df,test,data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apriori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df=train_df.merge(data, on='card_id', how='left')\n",
    "val_df=val_df.merge(data, on='card_id', how='left')\n",
    "train_df = train_df.set_index('merchant_id')['card_id'].rename('item_id')\n",
    "val_df = val_df.set_index('merchant_id')['card_id'].rename('item_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting order_item:                  78797\n",
      "Items with support >= 0.01:             856\n",
      "Remaining order_item:                 78797\n",
      "Remaining orders with 2+ items:        9171\n",
      "Remaining order_item:                 66291\n",
      "Item pairs:                             779\n",
      "Item pairs with support >= 0.01:        779\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/grecia/Dropbox/SKOLTECH/thesis/data_sets/ELO/Apriori.py:23: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  order_item = order_item.reset_index().as_matrix()\n"
     ]
    }
   ],
   "source": [
    "rules_i = Apriori.association_rules(train_df, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pairs_gen_i = Apriori.get_item_pairs(train_orders_i)\n",
    "train_pairs_i = Apriori.freq(train_pairs_gen_i).to_frame(\"freqAB\")\n",
    "train_pairs_i = train_pairs_i.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "train_pairs_i['pair'] = train_pairs_i.item_A.astype(str).str.cat(train_pairs_i.item_B.astype(str), sep='-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apriori(train, test, return_pred, num_cluster):\n",
    "    train_orders_i = train.set_index('merchant_id')['card_id'].rename('item_id')\n",
    "    test_orders_i = test.set_index('merchant_id')['card_id'].rename('item_id')\n",
    "\n",
    "    #item_name = train['product_id', 'product_name', 'aisle_id', 'department_id'].rename(columns={'product_id': 'item_id', 'product_name': 'item_name'})\n",
    "    rules_i = Apriori.association_rules(train_orders_i, 0.01)\n",
    "    #rules_final_i = Apriori.merge_item_name(rules_i, item_name).sort_values('lift', ascending=False)\n",
    "    #display(rules_final_i)\n",
    "\n",
    "    # Train set pairs\n",
    "    train_pairs_gen_i = Apriori.get_item_pairs(train_orders_i)\n",
    "    train_pairs_i = Apriori.freq(train_pairs_gen_i).to_frame(\"freqAB\")\n",
    "    train_pairs_i = train_pairs_i.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    train_pairs_i['pair'] = train_pairs_i.item_A.astype(str).str.cat(train_pairs_i.item_B.astype(str), sep='-')\n",
    "\n",
    "    # Test set pairs\n",
    "    test_pairs_gen_i = Apriori.get_item_pairs(test_orders_i)\n",
    "    test_pairs_i = Apriori.freq(test_pairs_gen_i).to_frame(\"freqAB\")\n",
    "    test_pairs_i = test_pairs_i.reset_index().rename(columns={'level_0': 'item_A', 'level_1': 'item_B'})\n",
    "    test_pairs_i['pair'] = test_pairs_i.item_A.astype(str).str.cat(test_pairs_i.item_B.astype(str), sep='-')\n",
    "\n",
    "    # Rules set pairs\n",
    "    rules_i['pair'] = rules_i.item_A.astype(str).str.cat(rules_i.item_B.astype(str), sep='-')\n",
    "\n",
    "    test_pair_set_i = set(np.unique(test_pairs_i.pair))\n",
    "    train_pair_set_i = set(np.unique(train_pairs_i.pair))\n",
    "    rules_pair_set_i = set(np.unique(rules_i.pair))\n",
    "\n",
    "    # TP= Pairs that exist in a priori pred and test\n",
    "    tp = len(list(test_pair_set_i & rules_pair_set_i))\n",
    "\n",
    "    # TN= pairs that exists train set but not in test\n",
    "    tn = len(list(test_pair_set_i - train_pair_set_i))\n",
    "\n",
    "    # FN= Pairs that exists in test but not in a priori\n",
    "    fn = len(list(rules_pair_set_i - test_pair_set_i))\n",
    "\n",
    "    # FP= Pairs that exists in a priori but not in test\n",
    "    fp = len(list(test_pair_set_i - rules_pair_set_i))\n",
    "\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * (recall * precision) / (recall + precision)\n",
    "    print('APRIORI')\n",
    "    return recall, precision, f1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
